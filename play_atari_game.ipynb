{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\manik\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\manik\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\manik\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\manik\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\manik\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\manik\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "class ConvNet:\n",
    "    \n",
    "    def __init__(self, params, trainable):\n",
    "        self.shape = [None, params.width, params.height, params.history_length]\n",
    "        self.x = tf.placeholder(tf.float32, self.shape)\n",
    "        self.in_dims = self.shape[1]*self.shape[2]*self.shape[3]\n",
    "        self.out_dims = params.actions\n",
    "        self.filters = [32, 64, 64] # convolution filters at each layer\n",
    "        self.num_layers = 3 # number of convolutional layers\n",
    "        self.filter_size = [8, 4, 4] # size at each layer\n",
    "        self.filter_stride = [4, 2, 1] # stride at each layer\n",
    "        self.fc_size = [512] # size of fully connected layers\n",
    "        self.fc_layers = 1 # number of fully connected layers\n",
    "        self.trainable = trainable\n",
    "\n",
    "        # dictionary for weights in network\n",
    "        self.weights = {}\n",
    "        # get predicted activation\n",
    "        self.y = self.infer(self.x)\n",
    "\n",
    "    def create_weight(self, shape):\n",
    "        init = tf.truncated_normal(shape, stddev=0.01)\n",
    "        return tf.Variable(init, name='weight')\n",
    "\n",
    "    def create_bias(self, shape):\n",
    "        init = tf.constant(0.01, shape=shape)\n",
    "        return tf.Variable(init, name='bias')\n",
    "\n",
    "    def create_conv2d(self, x, w, stride):\n",
    "        return tf.nn.conv2d(x, w, strides=[1, stride, stride, 1], padding='SAME')\n",
    "\n",
    "    def max_pool(self, x, size):\n",
    "        return tf.nn.max_pool(x, ksize=[1, size, size, 1], strides=[1, size, size, 1], padding='SAME')\n",
    "\n",
    "    def infer(self, _input):\n",
    "        self.layers = [_input]\n",
    "\n",
    "        # initialize convolution layers\n",
    "        for layer in range(self.num_layers):\n",
    "            with tf.variable_scope('conv' + str(layer)) as scope:\n",
    "                if layer == 0:\n",
    "                    in_channels = self.shape[-1]\n",
    "                    out_channels = self.filters[layer]\n",
    "                else:\n",
    "                    in_channels = self.filters[layer-1]\n",
    "                    out_channels = self.filters[layer]\n",
    "\n",
    "                shape = [ self.filter_size[layer], \n",
    "                          self.filter_size[layer],\n",
    "                          in_channels, \n",
    "                          out_channels ]\n",
    "\n",
    "                w = self.create_weight(shape)\n",
    "                conv = self.create_conv2d(self.layers[-1], w, self.filter_stride[layer])\n",
    "\n",
    "                b = self.create_bias([out_channels])\n",
    "                self.weights[w.name] = w\n",
    "                self.weights[b.name] = b\n",
    "                bias = tf.nn.bias_add(conv, b)\n",
    "                conv = tf.nn.relu(bias, name=scope.name)\n",
    "                self.layers.append(conv)\n",
    "\n",
    "        last_conv = self.layers[-1]\n",
    "\n",
    "        # flatten last convolution layer\n",
    "        dim = 1\n",
    "        for d in last_conv.get_shape()[1:].as_list():\n",
    "            dim *= d\n",
    "        reshape = tf.reshape(last_conv, [-1, dim], name='flat')\n",
    "        self.layers.append(reshape)\n",
    "\n",
    "        # initialize fully-connected layers\n",
    "        for layer in range(self.fc_layers):\n",
    "            with tf.variable_scope('hidden' + str(layer)) as scope:\n",
    "                if layer == 0:\n",
    "                    in_size = dim\n",
    "                else:\n",
    "                    in_size = self.fc_size[layer-1]\n",
    "\n",
    "                out_size = self.fc_size[layer]\n",
    "                shape = [in_size, out_size]\n",
    "                w = self.create_weight(shape)\n",
    "                b = self.create_bias([out_size])\n",
    "                self.weights[w.name] = w\n",
    "                self.weights[b.name] = b\n",
    "                hidden = tf.nn.relu_layer(self.layers[-1], w, b, name=scope.name)\n",
    "                self.layers.append(hidden)\n",
    "\n",
    "        # create last fully-connected layer\n",
    "        with tf.variable_scope('output') as scope:\n",
    "            in_size = self.fc_size[self.fc_layers - 1]\n",
    "            out_size = self.out_dims\n",
    "            shape = [in_size, out_size]\n",
    "            w = self.create_weight(shape)\n",
    "            b = self.create_bias([out_size])\n",
    "            self.weights[w.name] = w\n",
    "            self.weights[b.name] = b\n",
    "            hidden = tf.nn.bias_add(tf.matmul(self.layers[-1], w), b)\n",
    "            self.layers.append(hidden)\n",
    "\n",
    "        # return activation of the network\n",
    "        return self.layers[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Buffer:\n",
    "\n",
    "    def __init__(self, params):\n",
    "        history_length = params.history_length\n",
    "        width = params.width\n",
    "        height = params.height\n",
    "        self.dims = (width, height, history_length)\n",
    "        self.buffer = np.zeros(self.dims, dtype=np.uint8)\n",
    "\n",
    "    def add(self, state):\n",
    "        self.buffer[:, :, :-1] = self.buffer[:, :, 1:]\n",
    "        self.buffer[:, :, -1] = state\n",
    "\n",
    "    def getInput(self):\n",
    "        x = np.reshape(self.buffer, (1,)+ self.dims)\n",
    "        return x\n",
    "\n",
    "    def getState(self):\n",
    "        return self.buffer\n",
    "\n",
    "    def reset(self):\n",
    "        self.buffer.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "\n",
    "    def __init__(self, agent):\n",
    "        self.agent = agent\n",
    "        self.env = agent.env\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "    def run(self):\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.initialize_all_variables())\n",
    "            self.agent.randomRestart()\n",
    "\n",
    "            successes = 0\n",
    "            failures = 0\n",
    "            total_loss = 0\n",
    "\n",
    "            print(\"starting %d random plays to populate replay memory\" % self.agent.replay_start_size)\n",
    "            for i in range(self.agent.replay_start_size):\n",
    "                # follow random policy\n",
    "                state, action, reward, next_state, terminal = self.agent.observe(1)\n",
    "\n",
    "                if reward == 1:\n",
    "                    successes += 1\n",
    "                elif terminal:\n",
    "                    failures += 1\n",
    "\n",
    "                if (i+1) % 10000 == 0:\n",
    "                    print (\"\\nmemory size: %d\" % len(self.agent.memory),\\\n",
    "                          \"\\nSuccesses: \", successes,\\\n",
    "                          \"\\nFailures: \", failures)\n",
    "            \n",
    "            sample_success = 0\n",
    "            sample_failure = 0\n",
    "            print(\"\\nstart training...\")\n",
    "            start_time = time.time()\n",
    "            for i in range(self.agent.train_steps):\n",
    "                # annealing learning rate\n",
    "                lr = self.agent.trainEps(i)\n",
    "                state, action, reward, next_state, terminal = self.agent.observe(lr)\n",
    "\n",
    "                if len(self.agent.memory) > self.agent.batch_size and (i+1) % self.agent.update_freq == 0:\n",
    "                    sample_success, sample_failure, loss = self.agent.doMinibatch(sess, sample_success, sample_failure)\n",
    "                    total_loss += loss\n",
    "\n",
    "                if (i+1) % self.agent.steps == 0:\n",
    "                    self.agent.copy_weights(sess)\n",
    "\n",
    "                if reward == 1:\n",
    "                    successes += 1\n",
    "                elif terminal:\n",
    "                    failures += 1\n",
    "                \n",
    "                if ((i+1) % self.agent.save_weights == 0):\n",
    "                    self.agent.save(self.saver, sess, i+1)\n",
    "\n",
    "                if ((i+1) % self.agent.batch_size == 0):\n",
    "                    avg_loss = total_loss / self.agent.batch_size\n",
    "                    end_time = time.time()\n",
    "                    print (\"\\nTraining step: \", i+1,\\\n",
    "                          \"\\nmemory size: \", len(self.agent.memory),\\\n",
    "                          \"\\nLearning rate: \", lr,\\\n",
    "                          \"\\nSuccesses: \", successes,\\\n",
    "                          \"\\nFailures: \", failures,\\\n",
    "                          \"\\nSample successes: \", sample_success,\\\n",
    "                          \"\\nSample failures: \", sample_failure,\\\n",
    "                          \"\\nAverage batch loss: \", avg_loss,\\\n",
    "                          \"\\nBatch training time: \", (end_time-start_time)/self.agent.batch_size, \"s\")\n",
    "                    start_time = time.time()\n",
    "                    total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rand\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "class DQN:\n",
    "\n",
    "    def __init__(self, env, params):\n",
    "        self.env = env\n",
    "        params.actions = env.actions()\n",
    "        self.num_actions = env.actions()\n",
    "        self.episodes = params.episodes\n",
    "        self.steps = params.steps\n",
    "        self.train_steps = params.train_steps\n",
    "        self.update_freq = params.update_freq\n",
    "        self.save_weights = params.save_weights\n",
    "        self.history_length = params.history_length\n",
    "        self.discount = params.discount\n",
    "        self.eps = params.init_eps\n",
    "        self.eps_delta = (params.init_eps - params.final_eps) / params.final_eps_frame\n",
    "        self.replay_start_size = params.replay_start_size\n",
    "        self.eps_endt = params.final_eps_frame\n",
    "        self.random_starts = params.random_starts\n",
    "        self.batch_size = params.batch_size\n",
    "        self.ckpt_file = params.ckpt_dir+'/'+params.game\n",
    "\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        if params.lr_anneal:\n",
    "            self.lr = tf.train.exponential_decay(params.lr, self.global_step, params.lr_anneal, 0.96, staircase=True)\n",
    "        else:\n",
    "            self.lr = params.lr\n",
    "\n",
    "        self.buffer = Buffer(params)\n",
    "        self.memory = Memory(params.size, self.batch_size)\n",
    "\n",
    "        with tf.variable_scope(\"train\") as self.train_scope:\n",
    "            self.train_net = ConvNet(params, trainable=True)\n",
    "        with tf.variable_scope(\"target\") as self.target_scope:\n",
    "            self.target_net = ConvNet(params, trainable=False)\n",
    "\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(self.lr, params.decay_rate, 0.0, self.eps)\n",
    "\n",
    "        self.actions = tf.placeholder(tf.float32, [None, self.num_actions])\n",
    "        self.q_target = tf.placeholder(tf.float32, [None])\n",
    "        self.q_train = tf.reduce_max(tf.multiply(self.train_net.y, self.actions), reduction_indices=1)\n",
    "        self.diff = tf.subtract(self.q_target, self.q_train)\n",
    "\n",
    "        half = tf.constant(0.5)\n",
    "        if params.clip_delta > 0:\n",
    "            abs_diff = tf.abs(self.diff)\n",
    "            clipped_diff = tf.clip_by_value(abs_diff, 0, 1)\n",
    "            linear_part = abs_diff - clipped_diff\n",
    "            quadratic_part = tf.square(clipped_diff)\n",
    "            self.diff_square = tf.multiply(half, tf.add(quadratic_part, linear_part))\n",
    "        else:\n",
    "            self.diff_square = tf.multiply(half, tf.square(self.diff))\n",
    "\n",
    "        if params.accumulator == 'sum':\n",
    "            self.loss = tf.reduce_sum(self.diff_square)\n",
    "        else:\n",
    "            self.loss = tf.reduce_mean(self.diff_square)\n",
    "\n",
    "        # backprop with RMS loss\n",
    "        self.task = self.optimizer.minimize(self.loss, global_step=self.global_step)\n",
    "\n",
    "    def randomRestart(self):\n",
    "        self.env.restart()\n",
    "        for _ in range(self.random_starts):\n",
    "            action = rand.randrange(self.num_actions)\n",
    "            reward = self.env.act(action)\n",
    "            state = self.env.getScreen()\n",
    "            terminal = self.env.isTerminal()\n",
    "            self.buffer.add(state)\n",
    "\n",
    "            if terminal:\n",
    "                self.env.restart()\n",
    "\n",
    "    def trainEps(self, train_step):\n",
    "        if train_step < self.eps_endt:\n",
    "            return self.eps - train_step * self.eps_delta\n",
    "        else:\n",
    "            return self.eps_endt\n",
    "\n",
    "    def observe(self, exploration_rate):\n",
    "        if rand.random() < exploration_rate:\n",
    "            a = rand.randrange(self.num_actions)\n",
    "        else:\n",
    "            x = self.buffer.getInput()\n",
    "            action_values = self.train_net.y.eval( feed_dict={ self.train_net.x: x } )\n",
    "            a = np.argmax(action_values)\n",
    "        \n",
    "        state = self.buffer.getState()\n",
    "        action = np.zeros(self.num_actions)\n",
    "        action[a] = 1.0\n",
    "        reward = self.env.act(a)\n",
    "        screen = self.env.getScreen()\n",
    "        self.buffer.add(screen)\n",
    "        next_state = self.buffer.getState()\n",
    "        terminal = self.env.isTerminal()\n",
    "\n",
    "        reward = np.clip(reward, -1.0, 1.0)\n",
    "\n",
    "        self.memory.add(state, action, reward, next_state, terminal)\n",
    "        \n",
    "        \n",
    "        return state, action, reward, next_state, terminal\n",
    "\n",
    "    def doMinibatch(self, sess, successes, failures):\n",
    "        batch = self.memory.getSample()\n",
    "        state = np.array([batch[i][0] for i in range(self.batch_size)]).astype(np.float32)\n",
    "        actions = np.array([batch[i][1] for i in range(self.batch_size)]).astype(np.float32)\n",
    "        rewards = np.array([batch[i][2] for i in range(self.batch_size)]).astype(np.float32)\n",
    "        successes += np.sum(rewards==1)\n",
    "        next_state = np.array([batch[i][3] for i in range(self.batch_size)]).astype(np.float32)\n",
    "        terminals = np.array([batch[i][4] for i in range(self.batch_size)]).astype(np.float32)\n",
    "\n",
    "        failures += np.sum(terminals==1)\n",
    "        q_target = self.target_net.y.eval( feed_dict={ self.target_net.x: next_state } )\n",
    "        q_target_max = np.argmax(q_target, axis=1)\n",
    "        q_target = rewards + ((1.0 - terminals) * (self.discount * q_target_max))\n",
    "\n",
    "        (result, loss) = sess.run( [self.task, self.loss],\n",
    "                                    feed_dict={ self.q_target: q_target,\n",
    "                                                self.train_net.x: state,\n",
    "                                                self.actions: actions } )\n",
    "\n",
    "        return successes, failures, loss\n",
    "\n",
    "    def play(self):\n",
    "        self.randomRestart()\n",
    "        self.env.restart()\n",
    "        for i in range(self.episodes):\n",
    "            terminal = False\n",
    "            while not terminal:\n",
    "                #aca cambie algo\n",
    "                state, action, reward, screen, terminal = self.observe(self.eps)\n",
    "\n",
    "    def copy_weights(self, sess):\n",
    "        for key in self.train_net.weights.keys():\n",
    "            t_key = 'target/' + key.split('/', 1)[1]\n",
    "            sess.run(self.target_net.weights[t_key].assign(self.train_net.weights[key]))\n",
    "\n",
    "    def save(self, saver, sess, step):\n",
    "        saver.save(sess, self.ckpt_file, global_step=step)\n",
    "        \n",
    "    def restore(self, saver):\n",
    "        ckpt = tf.train.get_checkpoint_state(self.ckpt_file)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import cv2\n",
    "\n",
    "class Environment:\n",
    "\n",
    "    def __init__(self, params):\n",
    "        self.gym = gym.make(params.game)\n",
    "        self.observation = None\n",
    "        self.display = params.display\n",
    "        self.terminal = False\n",
    "        self.dims = (params.height, params.width)\n",
    "\n",
    "    def actions(self):\n",
    "        return self.gym.action_space.n\n",
    "\n",
    "    def restart(self):\n",
    "        self.observation = self.gym.reset()\n",
    "        self.terminal = False\n",
    "\n",
    "    def act(self, action):\n",
    "        if self.display:\n",
    "            self.gym.render()\n",
    "        self.observation, reward, self.terminal, info = self.gym.step(action)\n",
    "        if self.terminal:\n",
    "            #if self.display:\n",
    "            #    print \"No more lives, restarting\"\n",
    "            self.gym.reset()\n",
    "        return reward\n",
    "\n",
    "    def getScreen(self):\n",
    "        return cv2.resize(cv2.cvtColor(self.observation, cv2.COLOR_RGB2GRAY), self.dims)\n",
    "\n",
    "    def isTerminal(self):\n",
    "        return self.terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--game GAME] [--width WIDTH]\n",
      "                             [--height HEIGHT] [--size SIZE]\n",
      "                             [--history_length HISTORY_LENGTH] [--lr LR]\n",
      "                             [--lr_anneal LR_ANNEAL] [--discount DISCOUNT]\n",
      "                             [--batch_size BATCH_SIZE]\n",
      "                             [--accumulator ACCUMULATOR]\n",
      "                             [--decay_rate DECAY_RATE]\n",
      "                             [--min_decay_rate MIN_DECAY_RATE]\n",
      "                             [--init_eps INIT_EPS] [--final_eps FINAL_EPS]\n",
      "                             [--final_eps_frame FINAL_EPS_FRAME]\n",
      "                             [--clip_delta CLIP_DELTA] [--steps STEPS]\n",
      "                             [--train_steps TRAIN_STEPS]\n",
      "                             [--update_freq UPDATE_FREQ]\n",
      "                             [--replay_start_size REPLAY_START_SIZE]\n",
      "                             [--save_weights SAVE_WEIGHTS] [--display DISPLAY]\n",
      "                             [--random_starts RANDOM_STARTS]\n",
      "                             [--ckpt_dir CKPT_DIR] [--out OUT]\n",
      "                             [--episodes EPISODES] [--seed SEED]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\manik\\AppData\\Roaming\\jupyter\\runtime\\kernel-db7b8b5d-cb6d-4a09-9155-786af83f5638.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\manik\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3334: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import random as rand\n",
    "\n",
    "## these are our command line arguments.\n",
    "parser = argparse.ArgumentParser()\n",
    "envarg = parser.add_argument_group('Environment')\n",
    "envarg.add_argument(\"--game\", type=str, default=\"SpaceInvaders-v0\", help=\"Name of the atari game to test\")\n",
    "envarg.add_argument(\"--width\", type=int, default=84, help=\"Screen width\")\n",
    "envarg.add_argument(\"--height\", type=int, default=84, help=\"Screen height\")\n",
    "\n",
    "memarg = parser.add_argument_group('Memory')\n",
    "memarg.add_argument(\"--size\", type=int, default=100000, help=\"Memory size.\")\n",
    "memarg.add_argument(\"--history_length\", type=int, default=4, help=\"Number of most recent frames experiences by the agent.\")\n",
    "\n",
    "dqnarg = parser.add_argument_group('DQN')\n",
    "dqnarg.add_argument(\"--lr\", type=float, default=0.00025, help=\"Learning rate.\")\n",
    "dqnarg.add_argument(\"--lr_anneal\", type=float, default=20000, help=\"Step size of learning rate annealing.\")\n",
    "dqnarg.add_argument(\"--discount\", type=float, default=0.99, help=\"Discount rate.\")\n",
    "dqnarg.add_argument(\"--batch_size\", type=int, default=32, help=\"Batch size.\")\n",
    "dqnarg.add_argument(\"--accumulator\", type=str, default='mean', help=\"Batch accumulator.\")\n",
    "dqnarg.add_argument(\"--decay_rate\", type=float, default=0.95, help=\"Decay rate for RMSProp.\")\n",
    "dqnarg.add_argument(\"--min_decay_rate\", type=float, default=0.01, help=\"Min decay rate for RMSProp.\")\n",
    "dqnarg.add_argument(\"--init_eps\", type=float, default=1.0, help=\"Initial value of e in e-greedy exploration.\")\n",
    "dqnarg.add_argument(\"--final_eps\", type=float, default=0.1, help=\"Final value of e in e-greedy exploration.\")\n",
    "dqnarg.add_argument(\"--final_eps_frame\", type=float, default=1000000, help=\"The number of frames over which the initial value of e is linearly annealed to its final.\")\n",
    "dqnarg.add_argument(\"--clip_delta\", type=float, default=1, help=\"Clip error term in update between this number and its negative.\")\n",
    "dqnarg.add_argument(\"--steps\", type=int, default=10000, help=\"Copy main network to target network after this many steps.\")\n",
    "dqnarg.add_argument(\"--train_steps\", type=int, default=500000, help=\"Number of training steps.\")\n",
    "dqnarg.add_argument(\"--update_freq\", type=int, default=4, help=\"The number of actions selected between successive SGD updates.\")\n",
    "dqnarg.add_argument(\"--replay_start_size\", type=int, default=50000, help=\"A uniform random policy is run for this number of frames before learning starts and the resulting experience is used to populate the replay memory.\")\n",
    "dqnarg.add_argument(\"--save_weights\", type=int, default=10000, help=\"Save the mondel after this many steps.\")\n",
    "\n",
    "testarg = parser.add_argument_group('Test')\n",
    "testarg.add_argument(\"--display\", dest=\"display\", help=\"Display screen during testing.\")\n",
    "testarg.set_defaults(display=False)\n",
    "testarg.add_argument(\"--random_starts\", type=int, default=30, help=\"Perform max this number of no-op actions to be performed by the agent at the start of an episode.\")\n",
    "testarg.add_argument(\"--ckpt_dir\", type=str, default='model', help=\"Tensorflow checkpoint directory.\")\n",
    "testarg.add_argument(\"--out\", help=\"Output directory for gym.\")\n",
    "testarg.add_argument(\"--episodes\", type=int, default=100, help=\"Number of episodes.\")\n",
    "testarg.add_argument(\"--seed\", type=int, help=\"Random seed.\")\n",
    "args = parser.parse_args()\n",
    "if args.seed:\n",
    "    rand.seed(args.seed)\n",
    "if not os.path.exists(args.ckpt_dir):\n",
    "\tos.makedirs(args.ckpt_dir)\n",
    "\n",
    "#Checking for/Creating gym output directory\n",
    "if args.out:\n",
    "\tif not os.path.exists(args.out):\n",
    "\t\tos.makedirs(args.out)\n",
    "else:\n",
    "\tif not os.path.exists('gym-out/' + args.game):\n",
    "\t\tos.makedirs('gym-out/' + args.game)\n",
    "\targs.out = 'gym-out/' + args.game\n",
    "\n",
    "##Now let's train...\n",
    "\n",
    "#example of reinforcement learning in a game environment#\n",
    "#Q learning is a gernalised AI tech which builds a model of the environment \n",
    "#without prior knowledge via the use of a experenced relay\n",
    "#experienced relay is a automated learning technique where past experence \n",
    "#are incorporated into future models\n",
    "\n",
    "#initialise gym environment and dqn\n",
    "env = Environment(args)\n",
    "agent = DQN(env, args)\n",
    "\n",
    "# train agent\n",
    "Trainer(agent).run()\n",
    "\n",
    "# play the game\n",
    "env.gym.monitor.start(args.out, force=True)\n",
    "agent.play()\n",
    "env.gym.monitor.close()\n",
    "#run: python play_atari_game.py --display true\n",
    "#--display true allows you to view the game being played\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
