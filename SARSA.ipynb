{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rl'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-26aceb982c13>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mrl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAgent\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mrl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdqn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmean_q\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mrl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhuber_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'rl'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.callbacks import History\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Lambda\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from .rl.core import Agent\n",
    "from .rl.agents.dqn import mean_q\n",
    "from .rl.util import huber_loss\n",
    "from .rl.policy import EpsGreedyQPolicy, GreedyQPolicy\n",
    "from .rl.util import get_object_config\n",
    "\n",
    "\n",
    "class SARSAAgent(Agent):\n",
    "    \n",
    "    def __init__(self, model, nb_actions, policy=None, test_policy=None, gamma=.99, nb_steps_warmup=10,\n",
    "                 train_interval=1, delta_clip=np.inf, *args, **kwargs):\n",
    "        super(SarsaAgent, self).__init__(*args, **kwargs)\n",
    "\n",
    "        # Do not use defaults in constructor because that would mean that each instance shares the same\n",
    "        # policy.\n",
    "        if policy is None:\n",
    "            policy = EpsGreedyQPolicy()\n",
    "        if test_policy is None:\n",
    "            test_policy = GreedyQPolicy()\n",
    "\n",
    "        self.model = model\n",
    "        self.nb_actions = nb_actions\n",
    "        self.policy = policy\n",
    "        self.test_policy = test_policy\n",
    "        self.gamma = gamma\n",
    "        self.nb_steps_warmup = nb_steps_warmup\n",
    "        self.train_interval = train_interval\n",
    "\n",
    "        self.delta_clip = delta_clip\n",
    "        self.compiled = False\n",
    "        self.actions = None\n",
    "        self.observations = None\n",
    "        self.rewards = None\n",
    "\n",
    "    def compute_batch_q_values(self, state_batch):\n",
    "        batch = self.process_state_batch(state_batch)\n",
    "        q_values = self.model.predict_on_batch(batch)\n",
    "        assert q_values.shape == (len(state_batch), self.nb_actions)\n",
    "        return q_values\n",
    "\n",
    "    def compute_q_values(self, state):\n",
    "        q_values = self.compute_batch_q_values([state]).flatten()\n",
    "        assert q_values.shape == (self.nb_actions,)\n",
    "        return q_values\n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "        batch = np.array(batch)\n",
    "        if self.processor is None:\n",
    "            return batch\n",
    "        return self.processor.process_state_batch(batch)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(SarsaAgent, self).get_config()\n",
    "        config['nb_actions'] = self.nb_actions\n",
    "        config['gamma'] = self.gamma\n",
    "        config['nb_steps_warmup'] = self.nb_steps_warmup\n",
    "        config['train_interval'] = self.train_interval\n",
    "        config['delta_clip'] = self.delta_clip\n",
    "        config['model'] = get_object_config(self.model)\n",
    "        config['policy'] = get_object_config(self.policy)\n",
    "        config['test_policy'] = get_object_config(self.test_policy)\n",
    "        return config\n",
    "\n",
    "    def compile(self, optimizer, metrics=[]):\n",
    "        metrics += [mean_q]  # register default metrics\n",
    "\n",
    "        def clipped_masked_error(args):\n",
    "            y_true, y_pred, mask = args\n",
    "            loss = huber_loss(y_true, y_pred, self.delta_clip)\n",
    "            loss *= mask  # apply element-wise mask\n",
    "            return K.sum(loss, axis=-1)\n",
    "\n",
    "        # Create trainable model. The problem is that we need to mask the output since we only\n",
    "        # ever want to update the Q values for a certain action. The way we achieve this is by\n",
    "        # using a custom Lambda layer that computes the loss. This gives us the necessary flexibility\n",
    "        # to mask out certain parameters by passing in multiple inputs to the Lambda layer.\n",
    "        y_pred = self.model.output\n",
    "        y_true = Input(name='y_true', shape=(self.nb_actions,))\n",
    "        mask = Input(name='mask', shape=(self.nb_actions,))\n",
    "        loss_out = Lambda(clipped_masked_error, output_shape=(1,), name='loss')([y_pred, y_true, mask])\n",
    "        ins = [self.model.input] if type(self.model.input) is not list else self.model.input\n",
    "        trainable_model = Model(inputs=ins + [y_true, mask], outputs=[loss_out, y_pred])\n",
    "        assert len(trainable_model.output_names) == 2\n",
    "        combined_metrics = {trainable_model.output_names[1]: metrics}\n",
    "        losses = [\n",
    "            lambda y_true, y_pred: y_pred,  # loss is computed in Lambda layer\n",
    "            lambda y_true, y_pred: K.zeros_like(y_pred),  # we only include this for the metrics\n",
    "        ]\n",
    "        trainable_model.compile(optimizer=optimizer, loss=losses, metrics=combined_metrics)\n",
    "        self.trainable_model = trainable_model\n",
    "\n",
    "        self.compiled = True\n",
    "\n",
    "    def load_weights(self, filepath):\n",
    "        self.model.load_weights(filepath)\n",
    "\n",
    "    def save_weights(self, filepath, overwrite=False):\n",
    "        self.model.save_weights(filepath, overwrite=overwrite)\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.actions = collections.deque(maxlen=2)\n",
    "        self.observations = collections.deque(maxlen=2)\n",
    "        self.rewards = collections.deque(maxlen=2)\n",
    "        if self.compiled:\n",
    "            self.model.reset_states()\n",
    "\n",
    "    def forward(self, observation):\n",
    "        # Select an action.\n",
    "        q_values = self.compute_q_values([observation])\n",
    "        if self.training:\n",
    "            action = self.policy.select_action(q_values=q_values)\n",
    "        else:\n",
    "            action = self.test_policy.select_action(q_values=q_values)\n",
    "\n",
    "        # Book-keeping.\n",
    "        self.observations.append(observation)\n",
    "        self.actions.append(action)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def backward(self, reward, terminal):\n",
    "        metrics = [np.nan for _ in self.metrics_names]\n",
    "        if not self.training:\n",
    "            # We're done here. No need to update the experience memory since we only use the working\n",
    "            # memory to obtain the state over the most recent observations.\n",
    "            return metrics\n",
    "\n",
    "        # Train the network on a single stochastic batch.\n",
    "        if self.step > self.nb_steps_warmup and self.step % self.train_interval == 0:\n",
    "            # Start by extracting the necessary parameters (we use a vectorized implementation).\n",
    "            self.rewards.append(reward)\n",
    "            if len(self.observations) < 2:\n",
    "                return metrics  # not enough data yet\n",
    "\n",
    "            state0_batch = [self.observations[0]]\n",
    "            reward_batch = [self.rewards[0]]\n",
    "            action_batch = [self.actions[0]]\n",
    "            terminal1_batch = [0.] if terminal else [1.]\n",
    "            state1_batch = [self.observations[1]]\n",
    "            action1_batch = [self.actions[1]]\n",
    "\n",
    "            # Prepare and validate parameters.\n",
    "            state0_batch = self.process_state_batch(state0_batch)\n",
    "            state1_batch = self.process_state_batch(state1_batch)\n",
    "            terminal1_batch = np.array(terminal1_batch)\n",
    "            reward_batch = np.array(reward_batch)\n",
    "            assert reward_batch.shape == (1,)\n",
    "            assert terminal1_batch.shape == reward_batch.shape\n",
    "            assert len(action_batch) == len(reward_batch)\n",
    "\n",
    "            batch = self.process_state_batch(state1_batch)\n",
    "            q_values = self.compute_q_values(batch)\n",
    "            q_values = q_values.reshape((1, self.nb_actions))\n",
    "\n",
    "            q_batch = q_values[0, action1_batch]\n",
    "\n",
    "            assert q_batch.shape == (1,)\n",
    "            targets = np.zeros((1, self.nb_actions))\n",
    "            dummy_targets = np.zeros((1,))\n",
    "            masks = np.zeros((1, self.nb_actions))\n",
    "\n",
    "            # Compute r_t + gamma * Q(s_t+1, a_t+1)\n",
    "            discounted_reward_batch = self.gamma * q_batch\n",
    "            # Set discounted reward to zero for all states that were terminal.\n",
    "            discounted_reward_batch *= terminal1_batch\n",
    "            assert discounted_reward_batch.shape == reward_batch.shape\n",
    "            Rs = reward_batch + discounted_reward_batch\n",
    "            for idx, (target, mask, R, action) in enumerate(zip(targets, masks, Rs, action_batch)):\n",
    "                target[action] = R  # update action with estimated accumulated reward\n",
    "                dummy_targets[idx] = R\n",
    "                mask[action] = 1.  # enable loss for this specific action\n",
    "            targets = np.array(targets).astype('float32')\n",
    "            masks = np.array(masks).astype('float32')\n",
    "\n",
    "            # Finally, perform a single update on the entire batch. We use a dummy target since\n",
    "            # the actual loss is computed in a Lambda layer that needs more complex input. However,\n",
    "            # it is still useful to know the actual target to compute metrics properly.\n",
    "            state0_batch = state0_batch.reshape((1,) + state0_batch.shape)\n",
    "            ins = [state0_batch] if type(self.model.input) is not list else state0_batch\n",
    "            metrics = self.trainable_model.train_on_batch(ins + [targets, masks], [dummy_targets, targets])\n",
    "            metrics = [metric for idx, metric in enumerate(metrics) if idx not in (1, 2)]  # throw away individual losses\n",
    "            metrics += self.policy.metrics\n",
    "            if self.processor is not None:\n",
    "                metrics += self.processor.metrics\n",
    "        return metrics\n",
    "\n",
    "    @property\n",
    "    def layers(self):\n",
    "        return self.model.layers[:]\n",
    "\n",
    "    @property\n",
    "    def metrics_names(self):\n",
    "        # Throw away individual losses and replace output name since this is hidden from the user.\n",
    "        assert len(self.trainable_model.output_names) == 2\n",
    "        dummy_output_name = self.trainable_model.output_names[1]\n",
    "        model_metrics = [name for idx, name in enumerate(self.trainable_model.metrics_names) if idx not in (1, 2)]\n",
    "        model_metrics = [name.replace(dummy_output_name + '_', '') for name in model_metrics]\n",
    "\n",
    "        names = model_metrics + self.policy.metrics_names[:]\n",
    "        if self.processor is not None:\n",
    "            names += self.processor.metrics_names[:]\n",
    "        return names\n",
    "\n",
    "    @property\n",
    "    def policy(self):\n",
    "        return self.__policy\n",
    "\n",
    "    @policy.setter\n",
    "    def policy(self, policy):\n",
    "        self.__policy = policy\n",
    "        self.__policy._set_agent(self)\n",
    "\n",
    "    @property\n",
    "    def test_policy(self):\n",
    "        return self.__test_policy\n",
    "\n",
    "    @test_policy.setter\n",
    "    def test_policy(self, policy):\n",
    "        self.__test_policy = policy\n",
    "        self.__test_policy._set_agent(self)\n",
    "\n",
    "# Aliases\n",
    "SarsaAgent = SARSAAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
